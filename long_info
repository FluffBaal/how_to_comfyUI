# info
This is a longer version of the starter, if you want more information it can be found here. 


## Definitions of Key Terms

- **VAE (Variational Autoencoder)** – In Stable Diffusion, the VAE is responsible for converting images between **pixel space and latent space**. It encodes images into the model’s latent representation and decodes latents back into viewable images. In practical terms, the VAE compresses the image data and reconstructs it, influencing the color and clarity of outputs. Using an appropriate VAE can fix color saturation or detail issues in generated images.
    
- **LoRA (Low-Rank Adaptation)** – LoRA is an **efficient fine-tuning technique** for large models like Stable Diffusion. Instead of retraining a full model, LoRA injects small trainable matrices into the original model (often targeting the UNet and/or CLIP components) to adjust the model’s output. This means a LoRA file is **much smaller than a full model checkpoint** and **applies a style or specific concept** on top of a base model. For example, a LoRA can be trained to add a new art style or character; when applied, it alters the base model’s weights in a low-rank manner to achieve that style. LoRA models are **“patches” on top of base models**: they require the original model to be loaded, and then the LoRA’s changes are applied to steer the output.
    
- **Checkpoints** – In Stable Diffusion and ComfyUI, a “checkpoint” refers to a **model file containing the core neural network weights** (usually `.ckpt` or `.safetensors` files). A checkpoint typically includes **three main components**: the _UNet_ (the diffusion model that predicts noise->image in latent space), the _CLIP text encoder_ (which converts your text prompt into embeddings), and often a _VAE_ (for image encoding/decoding). Think of the checkpoint as the **base brain of the AI** – for example, the Stable Diffusion v1.5 checkpoint contains the learned knowledge of images and how to draw them. All image generation in ComfyUI starts with loading a checkpoint (base model) in the workflow. Checkpoints can be original base models (like SD1.5, SDXL) or fine-tuned models (like specialized art styles); they are stored in `ComfyUI/models/checkpoints`. ComfyUI can load checkpoints in various formats (ckpt, safetensors, Diffusers) and even **mix/merge models** if needed.
    
- **Base Models** – A base model refers to a **pretrained Stable Diffusion model** that serves as the foundation for further tuning. For example, Stable Diffusion 1.5 or SDXL are base models. LoRAs and other addons are generally trained to work with a particular base model. In context, applying a LoRA trained on SD1.5 means you should use an SD1.5-derived checkpoint as the base. Base models are basically the **full checkpoints** before any LoRA, hypernetwork, or textual inversion is applied. Compared to these add-ons, base models are large (gigabytes) and contain the full weights of the network.
    
- **Embeddings (Textual Inversion)** – In ComfyUI, embeddings are small **learned keyword representations** that can be loaded to influence the output. A textual inversion embedding is essentially a **compressed prompt** that the AI has been trained on. When you include the name of an embedding in your prompt, the model uses the _learned vector_ instead of trying to interpret that word normally. This helps simulate a specific art style or replicate a specific character or concept in outputs. Embeddings are usually placed in `ComfyUI/models/embeddings` and can be loaded similarly to other models. They act like **“micro models”** that get merged into the prompt’s effect.
    
- **Hypernetworks** – A hypernetwork is a **small neural network** that attaches to a Stable Diffusion model to alter its style without altering the main model weights. It typically hooks into the model’s internal layers (e.g. cross-attention) to modulate the output. Hypernetworks were an early way to fine-tune style (often for things like anime style, specific artist’s style, etc.) before LoRA became popular. In ComfyUI, you can load hypernetworks (files usually placed in a `hypernetworks` folder) and apply them to change the output style. They are listed as a supported model type in ComfyUI. Essentially, like LoRAs, hypernetworks are **add-ons** to the base model, but they function via a different mechanism (external network affecting the main model’s activations).
    

_(Other terms: “UNet” is the part of the model in the checkpoint doing the image diffusion; “CLIP” is the text encoder model in the checkpoint converting prompts to embeddings. “Sampler” refers to the algorithm (like Euler, DDIM, Karras) used in the KSampler node to generate the image from the model. ComfyUI’s node system abstracts these, but you will encounter them when building workflows.)_

## How to Use These Components in ComfyUI

Using ComfyUI involves constructing a **node graph (workflow)** that ties together your models (checkpoints, LoRAs, VAEs, etc.) and operations (sampling, upscaling, etc.). Below is a guide to using LoRAs, checkpoints/base models, and VAEs within ComfyUI.

### Using LoRA Adapters in ComfyUI

**Installation:** First, place your LoRA model files (often `.safetensors`) into the `ComfyUI/models/loras` folder. ComfyUI will auto-detect any LoRA files in that directory on startup.

**Loading in a workflow:** In the node graph, add a **“Load LoRA” node** (sometimes called _LoRaloader_). This node allows you to select which LoRA file to apply. You must connect it to a base model. Typically, you will have a **“Load Checkpoint” (base model)** node feeding into the **Load LoRA** node. The **Load LoRA node has inputs** for a model and a CLIP encoder – you connect the base model’s outputs (the UNet and CLIP from your checkpoint) into these inputs. The LoRA node also has a dropdown or text field to pick the `lora_name` (the file you placed in the folder).

**Adjusting strength:** The LoRA node provides sliders/inputs for `strength_model` and `strength_clip`. These control how strongly the LoRA influences the UNet (image generation) and the CLIP (prompt encoding) respectively. A higher strength makes the LoRA’s effect more pronounced. For example, if using a style LoRA, you might start with strength 1.0; increasing it might exaggerate the style, while lowering it blends it more subtly with the base model. You can tweak these to balance the effect on the visual output vs. how it interprets your prompt.

**Chaining multiple LoRAs:** ComfyUI allows using multiple LoRAs at once by chaining LoRA loader nodes. You can **connect the output of one Load LoRA node into another Load LoRA node** to apply multiple LoRA models sequentially. Each will further modify the model. This is useful if, for instance, you want to apply a character LoRA _and_ a style LoRA together. Just make sure to connect the model outputs in series and select different LoRA files in each node. All LoRA variants (LyCORIS/LoHa, etc.) use the same node type and method in ComfyUI – simply drop them in the `loras` folder and load via _Load LoRA_ nodes.

**Generating with LoRA:** Once your base model and LoRA nodes are set up and connected to the rest of the generation graph (text encoder, sampler, etc.), using them is straightforward. Ensure the **Load Checkpoint node is loading your desired base model** and the **Load LoRA node is set to your LoRA file**, then run the generation. For example, if you downloaded a base model `dreamshaper_8.safetensors` and a LoRA `blindbox_V1Mix.safetensors`, load them in the respective nodes. Then build the rest of the workflow (text encode, sampler, etc.), and hit **Queue (Ctrl+Enter)** to generate an image. The output will reflect the base model’s content _plus_ the LoRA’s stylistic or specific influence. (If the effect is too strong or weak, adjust the strengths and re-generate.)

### Applying Checkpoints and Base Models

**Installing models:** Place your Stable Diffusion checkpoint files (`.ckpt` or `.safetensors`) into the `ComfyUI/models/checkpoints` directory. ComfyUI will list models from this folder (and any extra paths configured) when you use a Load Checkpoint node. For example, to use SD1.5, you might put `v1-5-pruned-emaonly.safetensors` in the checkpoints folder.

**Loading base model in workflow:** Add a **“Load Checkpoint”** node in your ComfyUI graph. In its properties, select the desired model file (it will show a dropdown of filenames you placed). This node will load the model into memory. The output of the Load Checkpoint node typically provides the **model (UNet)** and the **CLIP text encoder** ready to use in the graph. You’ll connect these to other nodes: for example, connect the model output to your Sampler (KSampler) node’s model input, and the CLIP output to the text encoding or conditioning inputs.

**Base model vs. fine-tune:** If you plan to use LoRAs or other addons, make sure to **choose the appropriate base checkpoint**. For instance, if you want to apply a LoRA trained on Stable Diffusion 1.5, you should load an SD1.5-based checkpoint (like the official v1-5 model or a derivative) as your base. The Load Checkpoint node can be swapped at will – you can quickly try different base models by selecting a different checkpoint file in the dropdown and re-running the workflow, to see how the style/content changes.

**Generating with a checkpoint:** After the checkpoint is loaded and connected in the workflow, ensure all necessary parts of the graph are in place (e.g., a **CLIP Text Encode** node to handle your prompt, feeding into the diffusion model, etc., and finally a VAE decode to get an image). Then press **Queue** (or the play button) to generate the image. The first time a checkpoint loads, it might take a few seconds to initialize, but subsequent runs will reuse it until you change models. If the **Load Checkpoint node shows no models or “null,”** it means ComfyUI didn’t find your files – double-check the placement in the folder and restart ComfyUI.

**Optimizing base model usage:** ComfyUI can handle large models and even merge models if needed. If VRAM is a concern, ComfyUI has optimizations (like unloading models when not in use, or running on CPU with `--cpu` flag albeit slowly). You can also utilize the **Checkpoint Loader node’s** ability to load Diffusers format models if you prefer that. The key is to always start your workflow with the right checkpoint for the job – it’s the **“brain” of generation** to which all other modifiers (prompts, LoRAs, etc.) are applied.

### Configuring and Optimizing VAEs

**Using the default vs. custom VAE:** Many Stable Diffusion checkpoints come with a default VAE baked in. By default, when you load a checkpoint in ComfyUI, it will also load the associated VAE (if present) as part of the model. However, you might want to use a different VAE model for various reasons – for example, to fix color issues or to get a different tonal quality in images. ComfyUI allows you to **override the VAE** by using a dedicated **“Load VAE”** node.

**Installing a VAE model:** Download the VAE file you want (for instance, `vae-ft-mse-840000-ema-pruned.safetensors` or other VAE `.pt/.safetensors` files often shared in the community) and place it in `ComfyUI/models/vae`. ComfyUI scans the `vae` folder at startup. In your workflow, add a **Load VAE node**, and select the VAE file by name in that node’s properties.

**Using the VAE in workflow:** The VAE’s job is to encode and decode images to/from latents. In a typical text-to-image workflow, after the KSampler (which outputs a latent), you will have a **VAE Decode node** that converts the latent into an image. You should connect the **VAE model output from your Load VAE node into the VAE Decode node**. This ensures the custom VAE is used for decoding. (If you are encoding an initial image to a latent, e.g. in img2img, you’d also use the VAE for encoding via a VAE Encode node.) If you do not use a Load VAE node, the workflow will use the VAE that came with the checkpoint by default.

**Optimizing VAE selection:** To get the best results, use a VAE that matches your model or desired output. **For vivid colors and contrast**, many creators use VAEs like the _NAI (NovelAI) VAE_ or _Stable Diffusion v1.5 EMA VAE_, which can make outputs less washed-out. If your images look distorted or color-shifted, it might be a VAE mismatch – try a different one. ComfyUI lets you swap VAEs easily by selecting a different file in the Load VAE node and re-running the decode. The **Load VAE example** in documentation shows that you can pick a different VAE than the checkpoint’s default to influence the final result.

There’s also a configuration file (`extra_models_config.yaml`) where you can set default model paths or VAE paths if you want ComfyUI to auto-load a specific VAE with a given checkpoint, but the simplest method is using the node in your workflow for full control. In summary, **for each workflow run, you can decide which VAE to use** – either stick with the checkpoint’s built-in one or override it for adjusted output, which is a key way to **optimize image quality**.

## ComfyUI Manager

**What is ComfyUI Manager?** ComfyUI-Manager is an **extension (custom node) for ComfyUI** that greatly improves usability. It acts as a built-in manager for custom nodes and models, providing an interface to **install, remove, enable or disable** various extensions without leaving the UI. In addition, ComfyUI Manager includes a “hub” or library feature that lets you browse a wide range of custom nodes (from the community registry) and other resources directly from within ComfyUI. Essentially, it’s like a package manager for ComfyUI add-ons, saving you from manual downloads and installs for each custom node. The Manager also often links to documentation or info for nodes, making it easier to discover new capabilities.

**Installing ComfyUI Manager (manual ComfyUI installation):** If you have ComfyUI set up already (via the portable version or a Git clone), you can add the manager manually. The process is simple:

1. **Locate the `custom_nodes` folder** in your ComfyUI installation. (For example, if you installed ComfyUI in `C:\ComfyUI\`, go to `C:\ComfyUI\custom_nodes\`.)
2. **Clone the ComfyUI-Manager repository** into this folder. The command is:
    
    ```bash
    git clone https://github.com/ltdrdata/ComfyUI-Manager comfyui-manager
    ```
    
    This will create a subfolder `comfyui-manager` containing the manager extension code. (If you don’t have Git, you can also download the ZIP from GitHub and extract it into `custom_nodes/comfyui-manager`.)
3. **Restart ComfyUI.** When ComfyUI launches, it will detect the new manager extension. You should see new UI elements or menu options associated with ComfyUI-Manager.

After installation, ComfyUI-Manager typically adds a **“Nodes” or “Extensions” panel in the UI** (in the newer UI, it might appear as part of the side panel or a menu item). From there, you can browse the list of available custom nodes in the online registry and click to install them. The manager handles downloading and placing the files in the appropriate custom_nodes folders. It also lets you update or remove nodes with a click. This is immensely helpful, as ComfyUI has a thriving ecosystem of community nodes (for things like new samplers, integrations, etc.), and the manager spares you from doing each install manually.

**Integration and usage:** Once ComfyUI Manager is running, you can open its interface (often via a puzzle-piece icon or a menu). You’ll find sections to manage **“Installed” nodes (enable/disable them)** and **“Browse” the registry** for new nodes to add. For example, if you want to add a new Node that does some image processing, you can find it in the registry list and hit install – ComfyUI Manager will download it and prompt you to restart if needed. In short, ComfyUI Manager brings one-click extension management to ComfyUI, making it far easier to customize your setup.

_(Note: If you installed the new **ComfyUI Desktop** app (v1 beta), ComfyUI-Manager is already included by default. The manual steps above are mainly for those using a standalone or older version of ComfyUI.)_

## Workflows in ComfyUI

**What is a workflow?** In ComfyUI, a **workflow** is essentially the **node graph or diagram** you create to generate media. It’s a collection of nodes (each node is an operation or model) connected by links, forming a graph that defines the generation process. You can think of it as a visual program: the nodes and their connections determine how an image (or video, audio, etc.) is created. For example, a simple text-to-image workflow might include nodes for loading a model, encoding a prompt, sampling (diffusion steps), and decoding the image – all these connected make up the workflow. ComfyUI workflows are very flexible; you can branch into multiple steps, do things in parallel, chain post-processing, etc., which is why they’re often called graphs.

The **purpose of workflows** is to let you design and reuse generation pipelines. Rather than clicking through a fixed UI with limited options, ComfyUI gives you a canvas to **drag and drop nodes** and wire them up. This enables highly customized processes – from basic image generation to complex multi-stage enhancements. Once built, a workflow can be saved and shared.

**Workflow files (JSON and images):** Every ComfyUI workflow can be saved as a **JSON file** (which lists all the nodes, their settings, and connections). You can export your current graph by using **Ctrl+S** or the Save button, which produces a `.json` file. Interestingly, ComfyUI also allows embedding the workflow JSON into image metadata. When you generate an image, if you use the **Save Image** node, the output PNG/WebP can contain the entire workflow JSON inside it. That means you (or others) can **drag that image back into ComfyUI to reload the workflow** that created it. This is a very user-friendly way to share workflows: share the image itself, and others can get the exact setup by dragging it into their ComfyUI (or using the menu **Workflows -> Open** to load the image).

**Where to find existing workflows:** There are a few places to get pre-made workflows:

- **Official Examples/Templates:** The ComfyUI documentation and GitHub provide official workflow examples. The docs site has a section with **workflow templates** and basic examples (like a basic SD1.x text-to-image, an SDXL workflow, etc.). For instance, the official site links to **“official workflows”** that use only core nodes. These are great starting points. You might find these on the ComfyUI GitHub or documentation pages as downloadable images or JSON files. For example, ComfyUI’s examples GitHub pages include workflows for using LoRAs, using multiple checkpoints, upscaling, etc., all encoded in images that you can drag into the UI.
    
- **Community Hubs:** There is a thriving community sharing ComfyUI workflows. Websites like **ComfyUI Examples (comfyanonymous.github.io)** and **ComfyWorkflows.com** host user-contributed workflow files. On these sites, you can browse a gallery of images or descriptions and download the workflow (often as a PNG with metadata or a JSON). The community GitHub repo “Examples of ComfyUI workflows” also contains images with embedded workflows that demonstrate various complex setups. Additionally, users often share workflows on forums (like the /r/ComfyUI subreddit or Discord) and on model-sharing sites (CivitAI sometimes has workflow files for certain models).
    
- **Within ComfyUI (New UI):** If you are using a newer version or the desktop app with the updated interface, ComfyUI has features like **Template Workflows** built-in . These let you quickly load common workflow patterns from a menu, so you don’t have to reinvent the wheel for standard tasks. Templates might include things like “Text-to-Image (basic)”, “Image-to-Image”, etc., which you can then modify.
    

**How workflows are used:** To use an existing workflow, you typically **open the JSON or drag in the image**. ComfyUI will then populate the canvas with all the nodes and connections as saved. You might need to have the required models (checkpoint, LoRAs, etc. referenced in the workflow) in your folders so that the nodes recognize them. Once loaded, you can edit any node (change the prompt, swap the model, etc.) to fit your needs, then run the generation. Workflows save a lot of time, especially for complex setups like inpainting or outpainting, where multiple sub-processes are involved – you can load a community-provided workflow and just plug in your image or prompt.

**Example of a workflow:** A simple workflow JSON might contain nodes such as: _Load Checkpoint -> Set Latent Dimensions -> CLIP Text Encode (for prompt) -> CLIP Text Encode (for negative prompt) -> KSampler (does the diffusion steps) -> VAE Decode -> Save Image_. Each node has some parameters (like sampler steps, CFG scale, seed, etc. in KSampler). Visually, on the ComfyUI canvas, you’d see boxes (nodes) connected by lines, indicating how data flows (e.g., the model and text embeddings go into the KSampler, the KSampler’s output latent goes into VAE for decoding). The ComfyUI documentation describes this as a form of **visual programming** – the workflow graph is like your program. For instance, the **Basic SD1.x Workflow** example starts with a checkpoint node (the “brain” of generation) and ends with a VAE decode to produce an image. More complex workflows might branch into multiple samplers or include operations like upscaling, but the principle is the same.

In summary, workflows are **shareable, reusable blueprints** for AI generations. They allow both beginners and advanced users to replicate results and build on each other’s setups without manually setting every option each time.

## Desktop Version of ComfyUI

**Overview of ComfyUI Desktop:** ComfyUI Desktop is a **standalone packaged version** of ComfyUI (currently in Beta) offered for Windows and MacOS (Apple Silicon). It’s essentially ComfyUI bundled with its own Python environment and a new interface, delivered as a one-click install application. You can download an installer (for Windows with NVIDIA GPU, or a DMG for Mac M1/M2) from the official site. Being a packaged app, you don’t need to manually install Python, Git, or any dependencies – it comes with everything needed to run ComfyUI out-of-the-box.

**Key differences and benefits:**

- **Easy Installation & Updates:** The desktop app provides a **one-click installer** and is code-signed for security. It will automatically install all necessary dependencies on first run. Moreover, it has an **auto-update mechanism** – whenever a new stable release of ComfyUI or the Manager is out, the app can update itself, so you always have the latest features without manual pulling from Git.
    
- **Integrated ComfyUI Manager and UV**: The desktop version comes bundled with **ComfyUI-Manager** and the **uv** library out of the box. This means right away you have the node/package manager available (with the Custom Node Registry integration) without extra setup. In fact, the **Custom Node Registry (CNR)** which lists hundreds of custom nodes is directly integrated into the desktop app’s Manager panel. This is currently exclusive to the V1 desktop – it streamlines finding and installing extensions significantly.
    
- **New User Interface:** ComfyUI Desktop introduces a **brand new GUI** compared to the classic web-like interface. Notably, it features a **side menu bar** that provides quick access to your **queue/history, models, and node library**. It also includes a **node fuzzy search** (pressing double-click or a key to quickly search and add nodes by name), which speeds up workflow creation. Template workflows are accessible from the UI as well. Overall, the desktop UI is more polished and user-friendly, addressing some usability issues of the older interface (like easier navigation and model management panels).
    
- **Self-Contained Environment:** Because it’s a packaged Electron app, ComfyUI Desktop runs in its own environment. This means you don’t have to deal with Python environments or conflicts on your system. It also keeps all ComfyUI files (models, outputs, custom nodes) in a designated folder (for example on Windows, under your AppData or chosen directory). This isolation can be cleaner and makes uninstalling or moving easier. For users, it feels like using a normal application rather than running a server/script.
    
- **Cross-Platform Support:** While currently beta releases are for Windows (NVIDIA GPU required) and Mac (M-series), a Linux package is also planned or available in closed beta. This is beneficial for those on Mac especially, as previously one had to use Python and possibly set up a virtual environment; now there’s a native-feeling app.
    
- **Comparison to “portable” or web versions:** The traditional way to use ComfyUI was either running a local server (opening a browser for the UI) or using the Windows portable build. Those worked well but required more manual setup (downloading models separately, installing extensions manually, etc.). The **Desktop version streamlines this** by including common tools (Manager, registry, etc.) by default and presenting a refined UI. Feature-wise, the backend (the actual image generation capabilities) are the same ComfyUI core – you’re not losing any functionality. In fact, you gain early access to interface improvements. The **node system and workflows are fully compatible** between desktop and older versions, so you can share workflows between them.
    

In essence, ComfyUI Desktop is aimed at making ComfyUI **more accessible and convenient**. New users benefit from a quicker setup and a gentler learning curve with the template workflows and search. Advanced users benefit from integrated management and a tidier experience. If you’re already comfortable with manual ComfyUI, Desktop is not “different” in output, but you may enjoy the QoL enhancements (and you can run either one as needed, since they store data separately). As the desktop app is still in beta, minor quirks might exist, but it represents the future direction of ComfyUI – a unified, easy-to-use application for generative AI.

**Summary:** For installing and configuring ComfyUI, if you prefer full control and the latest bleeding-edge updates, a manual or portable setup works and lets you tinker (with ComfyUI Manager helping in that case). But the **desktop version offers a hassle-free, feature-rich experience** with minimal setup, ideal for most users who just want to get running quickly and have an integrated UI. It lowers the barrier to entry while packing the full power of ComfyUI under the hood. As the ComfyUI team puts it, it’s “the most powerful open source node-based application” for generative AI, now packaged in a user-friendly form, unlocking your creativity with less friction.
